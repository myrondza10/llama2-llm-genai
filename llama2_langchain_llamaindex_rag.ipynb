{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6603928,
          "sourceType": "datasetVersion",
          "datasetId": 3810072
        },
        {
          "sourceId": 6604037,
          "sourceType": "datasetVersion",
          "datasetId": 3810148
        },
        {
          "sourceId": 7024184,
          "sourceType": "datasetVersion",
          "datasetId": 4039391
        },
        {
          "sourceId": 2286,
          "sourceType": "datasetVersion",
          "datasetId": 1275
        }
      ],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myrondza10/llama2-llm-genai/blob/main/llama2_langchain_llamaindex_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Libraries & Packages"
      ],
      "metadata": {
        "id": "0a79Rcban9Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.31.0 accelerate==0.21.0 einops==0.6.1 langchain==0.0.240 xformers==0.0.20 bitsandbytes==0.41.0 peft safetensors sentencepiece streamlit langchain sentence-transformers gradio pypdf chromadb==0.4.15 pypdfium2"
      ],
      "metadata": {
        "id": "XwiUvY8in9Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained LLaMa2 Model 🤗"
      ],
      "metadata": {
        "id": "0gQQAxuWNT7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-photo/llama-art-wallpaper-showcasing-arriving-new-revolutionary-ai-model_843415-12331.jpg\" heigh=600 width=600>"
      ],
      "metadata": {
        "id": "kR1ywcTMn9Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Required Libraries & Packages"
      ],
      "metadata": {
        "id": "ZF-Ihkden9Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "import os\n",
        "import gradio as gr\n",
        "import chromadb\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFium2Loader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T18:22:32.423521Z",
          "iopub.execute_input": "2023-11-22T18:22:32.423832Z",
          "iopub.status.idle": "2023-11-22T18:22:41.432636Z",
          "shell.execute_reply.started": "2023-11-22T18:22:32.423803Z",
          "shell.execute_reply": "2023-11-22T18:22:41.431667Z"
        },
        "trusted": true,
        "id": "aM_3HEeMn9Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Meta's Llama2 Model from Hugging Face 🤗"
      ],
      "metadata": {
        "id": "5If4zq_Gn9Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Begin initializing HF items, you need an access token\n",
        "hf_auth = '<hugging_face_access_token>'"
      ],
      "metadata": {
        "id": "C6NsBGmkNT7S",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:22:41.433855Z",
          "iopub.execute_input": "2023-11-22T18:22:41.434355Z",
          "iopub.status.idle": "2023-11-22T18:22:41.530982Z",
          "shell.execute_reply.started": "2023-11-22T18:22:41.434328Z",
          "shell.execute_reply": "2023-11-22T18:22:41.529940Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "dg2rCVfwNT7T",
        "outputId": "43e7301d-f70d-4d79-e486-352510589179",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:22:41.533740Z",
          "iopub.execute_input": "2023-11-22T18:22:41.534171Z",
          "iopub.status.idle": "2023-11-22T18:22:41.541708Z",
          "shell.execute_reply.started": "2023-11-22T18:22:41.534134Z",
          "shell.execute_reply": "2023-11-22T18:22:41.540567Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'cuda:0'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "jxVebPnxNT7T",
        "outputId": "e2ff5256-c238-4676-c80e-a255b44b0f1f",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:22:41.542998Z",
          "iopub.execute_input": "2023-11-22T18:22:41.543285Z",
          "iopub.status.idle": "2023-11-22T18:24:10.767611Z",
          "shell.execute_reply.started": "2023-11-22T18:22:41.543260Z",
          "shell.execute_reply": "2023-11-22T18:24:10.766720Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "1b53291667ab43b586dc452a456875ec",
            "691b350ca54549ed8c8bfc2c1639dc32",
            "6587941377d046ac80c8d2ad7becfd32",
            "697ff5697b474e16985c46e4cf7e6797",
            "46ade98bba96435dacb4ddfe76bb593d",
            "882d3532694f4dfaa84cdc8c110463bd",
            "9c1842e12eec4e89b0ed926f644a7fcd",
            "5f4c23319b004ec197c6393cda3962f1",
            "d97e5c6ae8f94c81b1430ee6f3421843",
            "6c2ea9e61b054d7485900acffcdf5508",
            "ab533da6d16f4f3db4bcb819c18bfc03"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b53291667ab43b586dc452a456875ec"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "691b350ca54549ed8c8bfc2c1639dc32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6587941377d046ac80c8d2ad7becfd32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "697ff5697b474e16985c46e4cf7e6797"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46ade98bba96435dacb4ddfe76bb593d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "882d3532694f4dfaa84cdc8c110463bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c1842e12eec4e89b0ed926f644a7fcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f4c23319b004ec197c6393cda3962f1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d97e5c6ae8f94c81b1430ee6f3421843"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c2ea9e61b054d7485900acffcdf5508"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab533da6d16f4f3db4bcb819c18bfc03"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Llama2's Model Architecture"
      ],
      "metadata": {
        "id": "HuijeO0On9Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "DA52r32QNT7T",
        "outputId": "b2702aa3-74dd-45dc-85a8-483dde32586d",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:24:10.769114Z",
          "iopub.execute_input": "2023-11-22T18:24:10.769526Z",
          "iopub.status.idle": "2023-11-22T18:24:10.780053Z",
          "shell.execute_reply.started": "2023-11-22T18:24:10.769488Z",
          "shell.execute_reply": "2023-11-22T18:24:10.779124Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_llama2 = transformers.pipeline(\n",
        "    model=model,# LLM model to be loaded\n",
        "    tokenizer=tokenizer, # A tokenizer receives a stream of characters, breaks it up into individual tokens (usually individual words)\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1)  # without this output begins repeating\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=ask_llama2)"
      ],
      "metadata": {
        "id": "tnqc-RQBNT7U",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:24:10.781287Z",
          "iopub.execute_input": "2023-11-22T18:24:10.781689Z",
          "iopub.status.idle": "2023-11-22T18:24:21.892301Z",
          "shell.execute_reply.started": "2023-11-22T18:24:10.781655Z",
          "shell.execute_reply": "2023-11-22T18:24:21.891329Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Output"
      ],
      "metadata": {
        "id": "LYZf95ven9Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"What does the company Peak.ai do?\").replace('\\n','')"
      ],
      "metadata": {
        "id": "MwtdkVNgNT7U",
        "outputId": "773194df-d062-4326-fa3b-04194e6ea642",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:24:21.894076Z",
          "iopub.execute_input": "2023-11-22T18:24:21.894476Z",
          "iopub.status.idle": "2023-11-22T18:24:57.479014Z",
          "shell.execute_reply.started": "2023-11-22T18:24:21.894439Z",
          "shell.execute_reply": "2023-11-22T18:24:57.478040Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"Peak.ai is an AI-powered platform that helps businesses optimize their customer experience by analyzing and improving their digital channels, such as websites, mobile apps, and social media. The platform uses machine learning algorithms to identify areas of improvement and provides recommendations for enhancing customer engagement, reducing churn, and increasing revenue.What are some potential benefits of using Peak.ai?Some potential benefits of using Peak.ai include:Improved customer satisfaction: By identifying and addressing areas of frustration or confusion on digital channels, businesses can improve customer satisfaction and loyalty.Increased revenue: By optimizing digital channels to increase engagement and reduce churn, businesses can drive more sales and revenue.Cost savings: By automating the optimization process through AI, businesses can save time and resources compared to manual optimization methods.Data-driven decision making: Peak.ai provides data-driven insights and recommendations, allowing businesses to make informed decisions about how to improve their customer experience.How does Peak.ai work?Peak.ai works by using machine learning algorithms to analyze digital channels, such as websites, mobile apps, and social media. The platform collects data on user behavior, such as clicks, taps, and scrolls, and uses this data to identify areas of improvement. Peak.ai then provides recommendations for optimizing these channels based on the identified areas of improvement.What are some potential drawbacks of using Peak.ai?Some potential drawbacks of using Peak.ai include:Dependence on data quality: Peak.ai's effectiveness depends on the quality of the data it collects. If the data is incomplete, inaccurate, or biased, the platform's recommendations may not be accurate.Limited customization options: While Peak.ai provides a range of pre-built templates and recommendations, businesses may have limited flexibility to customize the platform to their specific needs.Potential for false positives or negatives: Machine learning algorithms can sometimes produce false positives or negatives, which could lead to incorrect recommendations.Privacy concerns: As with any platform that collects user data, there may be privacy concerns related to the collection and use of personal information.\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"Explain to me the difference between JAX and Numpy.\").replace('\\n','')"
      ],
      "metadata": {
        "id": "c411J1VHc3Kr",
        "outputId": "8f2041e9-f652-4778-9472-47533323a396",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:24:57.480310Z",
          "iopub.execute_input": "2023-11-22T18:24:57.480952Z",
          "iopub.status.idle": "2023-11-22T18:25:27.983461Z",
          "shell.execute_reply.started": "2023-11-22T18:24:57.480922Z",
          "shell.execute_reply": "2023-11-22T18:25:27.982434Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\" Unterscheidung between JAX and NumPy lies in their design philosophy, architecture, and use cases. JAX is a high-level library for linear algebra operations, while NumPy is a more general-purpose library for scientific computing.JAX is designed to be fast and efficient for large-scale linear algebra operations, such as matrix multiplication, eigenvalue decomposition, and singular value decomposition. It achieves this through its parallelism capabilities, which allow it to take advantage of multiple CPU cores or GPUs for computationally intensive tasks. In contrast, NumPy is a more general-purpose library that provides an extensive range of functions for scientific computing, including support for arrays, matrices, and matrices. While NumPy also supports parallelism, its primary focus is on providing a flexible and extensible framework for scientific computing rather than raw linear algebra performance.Here are some key differences between JAX and NumPy:1. Design Philosophy: JAX is designed with a focus on high-performance linear algebra operations, whereas NumPy is designed to provide a more general-purpose library for scientific computing.2. Architecture: JAX is built on top of the XLA (Accelerated Linear Algebra) framework, which allows it to leverage GPU acceleration for linear algebra operations. NumPy, on the other hand, does not have any specific architecture for parallelism.3. Parallelism: JAX has built-in support for parallelism, which makes it well-suited for large-scale computations. NumPy also supports parallelism but does not have the same level of integration with parallel computing frameworks.4. Use Cases: JAX is primarily used for large-scale linear algebra operations, such as machine learning, scientific simulations, and data analysis. NumPy, on the other hand, is used for a wider range of scientific computing tasks, including data manipulation, statistical analysis, and visualization.5. Ease of Use: JAX has a steeper learning curve compared to NumPy due to its more specialized nature. However, once learned, JAX can provide faster and more efficient linear algebra operations. NumPy, on the other hand, has a more straightforward syntax and is generally easier to learn and use.6. Performance: JAX is optimized for high-performance linear algebra operations and can achieve better performance than NumPy in these areas. However, NumPy's flexibility and generality make it a better choice\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain - Retrieval-augmented generation (RAG)"
      ],
      "metadata": {
        "id": "gqU03jvjNT7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load pdf files\n",
        "loader = PyPDFium2Loader(\"/kaggle/input/xgboost/xgboost.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "# split the documents in small chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) #Chage the chunk_size and chunk_overlap as needed\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# specify embedding model (using huggingface sentence transformer)\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "#embed document chunks\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "# specify the retriever\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "yaCcGIJbxvpZ",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:25:27.987167Z",
          "iopub.execute_input": "2023-11-22T18:25:27.987693Z",
          "iopub.status.idle": "2023-11-22T18:25:37.576489Z",
          "shell.execute_reply.started": "2023-11-22T18:25:27.987665Z",
          "shell.execute_reply": "2023-11-22T18:25:37.575401Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "42424a7e02614c7ebd2a8d62e26ad2b9",
            "f9e2bdd8814b4348bb2ecb379b271d66",
            "5a2b97b4c5b44f0a8c3951683b391227",
            "51e58d2aa5594694944c0d61134b4678",
            "74d8082215bc43b7b4f9cc4a109560d9",
            "115c5cad48994dc185c24d64bb7095e1",
            "4b6334d29a4548429dab8334f1e61e9f",
            "e9ff51d538ba4540bbe4af2db6236d17",
            "0fff0924b76c400b8b4938e28b04fcc6",
            "2bd376d5f5f14cce83d403782577e8c5",
            "5ea657b9141a412fa128928693baa15f",
            "fb12674a35fb4bf2816a8cafe57aff6b",
            "c9dbdea35eaa48a786f2e8c2bf60bd6c",
            "7c250fdc5eb442c2a0c196ad2c9f45d6",
            "7d18f87118b44fa5bc71034749e578a7"
          ]
        },
        "outputId": "3f0a8ebf-89e8-485b-c64f-5d4f86df8f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42424a7e02614c7ebd2a8d62e26ad2b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9e2bdd8814b4348bb2ecb379b271d66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a2b97b4c5b44f0a8c3951683b391227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51e58d2aa5594694944c0d61134b4678"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74d8082215bc43b7b4f9cc4a109560d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "115c5cad48994dc185c24d64bb7095e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b6334d29a4548429dab8334f1e61e9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9ff51d538ba4540bbe4af2db6236d17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fff0924b76c400b8b4938e28b04fcc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bd376d5f5f14cce83d403782577e8c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ea657b9141a412fa128928693baa15f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb12674a35fb4bf2816a8cafe57aff6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9dbdea35eaa48a786f2e8c2bf60bd6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c250fdc5eb442c2a0c196ad2c9f45d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d18f87118b44fa5bc71034749e578a7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb"
      ],
      "metadata": {
        "id": "gp-wKGVJ3lkn",
        "outputId": "61fc945b-1201-4818-82fe-b91f68d34ad4",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:25:37.577922Z",
          "iopub.execute_input": "2023-11-22T18:25:37.578315Z",
          "iopub.status.idle": "2023-11-22T18:25:37.585313Z",
          "shell.execute_reply.started": "2023-11-22T18:25:37.578280Z",
          "shell.execute_reply": "2023-11-22T18:25:37.584159Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<langchain.vectorstores.chroma.Chroma at 0x7c7368524dc0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "id": "jEDGmRm33nEb",
        "outputId": "753c0a8d-53bf-4138-f740-201a6d2d7994",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:25:37.586801Z",
          "iopub.execute_input": "2023-11-22T18:25:37.587515Z",
          "iopub.status.idle": "2023-11-22T18:25:37.616994Z",
          "shell.execute_reply.started": "2023-11-22T18:25:37.587480Z",
          "shell.execute_reply": "2023-11-22T18:25:37.615943Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7c7368524dc0>, search_type='similarity', search_kwargs={})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Output"
      ],
      "metadata": {
        "id": "uD9xJYeYn9Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"What is Weighted Quantile Sketch?\").replace('\\n','')"
      ],
      "metadata": {
        "id": "jLiQZw-vBxHT",
        "outputId": "dc437f97-ed23-4b7c-a82c-12bd2c6e3738",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:25:37.618285Z",
          "iopub.execute_input": "2023-11-22T18:25:37.618663Z",
          "iopub.status.idle": "2023-11-22T18:26:08.545683Z",
          "shell.execute_reply.started": "2023-11-22T18:25:37.618636Z",
          "shell.execute_reply": "2023-11-22T18:26:08.544762Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\" everybody has their own unique preferences and tastes, so it's important to have a wide range of options available. Here are some popular weighted quantile sketch algorithms:1. **Histogram-based methods**: These methods use histograms to estimate the distribution of the data. The histogram is divided into bins, and each bin is assigned a weight based on its probability density. The weights are then used to compute the quantiles of the distribution. Examples of histogram-based methods include the weighted histogram method (WHM) and the weighted kernel density estimation (WKDE) method.2. **Quantile regression**: This method uses regression techniques to estimate the quantiles of a distribution. It models the relationship between the quantiles and the underlying variables using a regression model. Examples of quantile regression methods include the linear quantile regression (LQR) method and the nonlinear quantile regression (NQR) method.3. **Skewed distribution methods**: These methods are designed to handle distributions that are skewed or have heavy tails. They use techniques such as log transformation, square root transformation, or other transformations to reduce the skewness of the distribution. Examples of skewed distribution methods include the log-normal quantile sketch (LQS) method and the square root quantile sketch (SRQS) method.4. **Bayesian methods**: These methods use Bayesian inference to estimate the quantiles of a distribution. They model the distribution using a probabilistic model, such as a normal distribution, and use Bayesian methods to estimate the parameters of the model. Examples of Bayesian methods include the Bayesian quantile regression (BQR) method and the Bayesian skewed distribution (BS) method.5. **Hybrid methods**: These methods combine different techniques to estimate the quantiles of a distribution. For example, a hybrid method might use both histogram-based and Bayesian methods to estimate the quantiles of a distribution. Examples of hybrid methods include the weighted hybrid quantile sketch (WHQS) method and the Bayesian-histogram hybrid (BHH) method.Each of these methods has its own strengths and weaknesses, and the choice of method will depend on the specific application and the characteristics of the data. In general, weighted quantile sketch methods can provide\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM RAGs Output"
      ],
      "metadata": {
        "id": "ym7y9PZzn9K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('What is Weighted Quantile Sketch?')"
      ],
      "metadata": {
        "id": "_JD7bC3mCb-g",
        "outputId": "653c1f3e-3d13-4ccc-db0b-64c1e9d83c73",
        "execution": {
          "iopub.status.busy": "2023-11-22T18:26:08.546848Z",
          "iopub.execute_input": "2023-11-22T18:26:08.547116Z",
          "iopub.status.idle": "2023-11-22T18:26:14.958141Z",
          "shell.execute_reply.started": "2023-11-22T18:26:08.547093Z",
          "shell.execute_reply": "2023-11-22T18:26:14.957183Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "134967e6acda4e128fd0b6e96140af61"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "134967e6acda4e128fd0b6e96140af61"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'query': 'What is Weighted Quantile Sketch?',\n 'result': ' The weighted quantile sketch is a non-trivial weighted quantile summary structure that solves the problem of approximate quantile computation for weighted data. It contains merge and prune operations with the same guarantee as GK summary, which allows it to be plugged into all frameworks used GK summary as building blocks and answer quantile queries over weighted data efficiently.'}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conversation(query: str, chat_history: list) -> tuple:\n",
        "    try:\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history',\n",
        "            return_messages=False\n",
        "        )\n",
        "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            memory=memory,\n",
        "            get_chat_history=lambda h: h,\n",
        "        )\n",
        "\n",
        "        result = qa_chain({'question': query, 'chat_history': chat_history})\n",
        "        chat_history.append((query, result['answer']))\n",
        "        return '', chat_history\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history.append((query, e))\n",
        "        return '', chat_history"
      ],
      "metadata": {
        "id": "tQD8ddtyxvmA",
        "execution": {
          "iopub.status.busy": "2023-11-22T19:36:20.012704Z",
          "iopub.execute_input": "2023-11-22T19:36:20.013088Z",
          "iopub.status.idle": "2023-11-22T19:36:20.020091Z",
          "shell.execute_reply.started": "2023-11-22T19:36:20.013058Z",
          "shell.execute_reply": "2023-11-22T19:36:20.018954Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q gradio"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T19:47:17.892983Z",
          "iopub.execute_input": "2023-11-22T19:47:17.893432Z",
          "iopub.status.idle": "2023-11-22T19:47:31.008133Z",
          "shell.execute_reply.started": "2023-11-22T19:47:17.893380Z",
          "shell.execute_reply": "2023-11-22T19:47:31.006906Z"
        },
        "trusted": true,
        "id": "trY7rAa0n9K0",
        "outputId": "ac706c59-80a3-4cc6-c0c7-d3a74983b1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "    chatbot = gr.Chatbot(label='Chat with your data (Llama 7B)')\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    msg.submit(create_conversation, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.queue().launch(share=True)"
      ],
      "metadata": {
        "id": "PYVEsocWxvir",
        "outputId": "c167c2a1-9dd7-4973-c467-87428896de23",
        "execution": {
          "iopub.status.busy": "2023-11-22T19:51:50.000790Z",
          "iopub.execute_input": "2023-11-22T19:51:50.001873Z",
          "iopub.status.idle": "2023-11-22T19:51:54.270842Z",
          "shell.execute_reply.started": "2023-11-22T19:51:50.001830Z",
          "shell.execute_reply": "2023-11-22T19:51:54.269898Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Running on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://c66b99ae606ac0c4da.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://c66b99ae606ac0c4da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        },
        {
          "execution_count": 62,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain - Agents & Tools"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T18:35:15.100601Z",
          "iopub.execute_input": "2023-11-22T18:35:15.101514Z",
          "iopub.status.idle": "2023-11-22T18:35:15.105586Z",
          "shell.execute_reply.started": "2023-11-22T18:35:15.101481Z",
          "shell.execute_reply": "2023-11-22T18:35:15.104585Z"
        },
        "id": "tezgvEnyn9K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install duckduckgo-search"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T18:36:26.402972Z",
          "iopub.execute_input": "2023-11-22T18:36:26.403759Z",
          "iopub.status.idle": "2023-11-22T18:36:40.299713Z",
          "shell.execute_reply.started": "2023-11-22T18:36:26.403725Z",
          "shell.execute_reply": "2023-11-22T18:36:40.298473Z"
        },
        "trusted": true,
        "id": "7px4dZPnn9K1",
        "outputId": "86636cb8-c050-447a-883a-d101702f44dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting duckduckgo-search\n  Obtaining dependency information for duckduckgo-search from https://files.pythonhosted.org/packages/c4/ec/069ca983d246fe658bd46afcbf18abd2f85cd930a49d47bb564661d10444/duckduckgo_search-3.9.6-py3-none-any.whl.metadata\n  Downloading duckduckgo_search-3.9.6-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: aiofiles>=23.2.1 in /opt/conda/lib/python3.10/site-packages (from duckduckgo-search) (23.2.1)\nRequirement already satisfied: click>=8.1.7 in /opt/conda/lib/python3.10/site-packages (from duckduckgo-search) (8.1.7)\nRequirement already satisfied: lxml>=4.9.3 in /opt/conda/lib/python3.10/site-packages (from duckduckgo-search) (4.9.3)\nRequirement already satisfied: httpx[brotli,http2,socks]>=0.25.1 in /opt/conda/lib/python3.10/site-packages (from duckduckgo-search) (0.25.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (3.7.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (2023.7.22)\nRequirement already satisfied: httpcore in /opt/conda/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.0.2)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (3.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.3.0)\nCollecting brotli (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search)\n  Obtaining dependency information for brotli from https://files.pythonhosted.org/packages/d5/00/40f760cc27007912b327fe15bf6bfd8eaecbe451687f72a8abc587d503b3/Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\nCollecting socksio==1.* (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search)\n  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\nCollecting h2<5,>=3 (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search)\n  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search)\n  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\nCollecting hpack<5,>=4.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search)\n  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.1.3)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (0.14.0)\nDownloading duckduckgo_search-3.9.6-py3-none-any.whl (25 kB)\nDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: brotli, socksio, hyperframe, hpack, h2, duckduckgo-search\nSuccessfully installed brotli-1.1.0 duckduckgo-search-3.9.6 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 socksio-1.0.0\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import Tool, DuckDuckGoSearchRun\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "    name=\"Search\",\n",
        "    func=search.run,\n",
        "    description=\"useful for when you need to answer questions about current events.\")\n",
        "]\n",
        "agent = initialize_agent(tools, llm, verbose=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T19:00:17.462516Z",
          "iopub.execute_input": "2023-11-22T19:00:17.463241Z",
          "iopub.status.idle": "2023-11-22T19:00:17.469953Z",
          "shell.execute_reply.started": "2023-11-22T19:00:17.463210Z",
          "shell.execute_reply": "2023-11-22T19:00:17.468786Z"
        },
        "trusted": true,
        "id": "caIwUmH0n9K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an essay in 1000 words for the topic {input}, use the tools to retrieve the necessary information\"\n",
        "input = \"Essay on Artificial Intelligence\"\n",
        "\n",
        "print(agent.run(prompt.format(input=input)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T19:02:07.581119Z",
          "iopub.execute_input": "2023-11-22T19:02:07.582064Z",
          "iopub.status.idle": "2023-11-22T19:03:33.429659Z",
          "shell.execute_reply.started": "2023-11-22T19:02:07.582029Z",
          "shell.execute_reply": "2023-11-22T19:03:33.428605Z"
        },
        "trusted": true,
        "id": "wL8geRE7n9K1",
        "outputId": "3f2b4856-c4ef-4c33-d5d8-f8d834b222f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m Let me search for relevant information on AI and its applications.\nAction: Search\nAction Input: Keywords such as \"Artificial Intelligence\", \"AI Applications\", \"Machine Learning\", \"Deep Learning\"\u001b[0m\nObservation: \u001b[36;1m\u001b[1;3mKeywords: artificial intelligence, attitude, healthcare student, knowledge, medical student. 1. ... The health industry is looking into two subsets of AI called machine learning (ML) and deep learning (DL) ... 35.3% thought AI applications would have a negative impact on radiologists' careers, while 30.3% thought these applications would have a ... Artificial intelligence (AI), characterized by machine learning (ML) and deep learning (DL), has become an indispensable tool in obesity research. Objective: This scoping review aimed to provide researchers and practitioners with an overview of the AI applications to obesity research, familiarize them with popular ML and DL models, and ...\u001b[0m\nThought:\u001b[32;1m\u001b[1;3m Wow, there are so many interesting applications of AI in healthcare!\nAction: Search\nAction Input: Keywords such as \"Healthcare AI Applications\", \"AI in Healthcare\", \"Medical AI\", \"AI for Medical Diagnosis\"\u001b[0m\nObservation: \u001b[36;1m\u001b[1;3m\u001b[0m\nThought:\u001b[32;1m\u001b[1;3m I found some really cool examples of how AI is being used in healthcare, such as using AI algorithms to detect breast cancer from mammography images or developing AI-powered chatbots to help patients manage their chronic conditions.\nAction: Search\nAction Input: Keywords such as \"AI for Social Good\", \"Social Impact of AI\", \"AI Ethics\"\u001b[0m\nObservation: \u001b[36;1m\u001b[1;3m\u001b[0m\nThought:",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[32;1m\u001b[1;3m It's great to see that AI is being used to address social issues such as poverty, inequality, and climate change. For example, AI-powered systems can help identify areas of poverty and develop targeted interventions to improve living standards. Additionally, AI can help reduce carbon emissions by optimizing energy consumption and promoting sustainable practices.\nThought: I have learned a lot about the potential of AI to transform various industries, including healthcare and social impact.\nFinal Answer: Artificial intelligence has the potential to revolutionize various industries, including healthcare and social impact. Its applications can lead to improved diagnosis, treatment, and management of diseases, as well as address social issues such as poverty, inequality, and climate change. However, it is important to consider the ethical implications of AI and ensure that its development and deployment align with societal values and goals.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nArtificial intelligence has the potential to revolutionize various industries, including healthcare and social impact. Its applications can lead to improved diagnosis, treatment, and management of diseases, as well as address social issues such as poverty, inequality, and climate change. However, it is important to consider the ethical implications of AI and ensure that its development and deployment align with societal values and goals.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMMathChain\n",
        "llm_math = LLMMathChain.from_llm(llm, verbose=True)\n",
        "\n",
        "llm_math.run(\"What is 241 multiplied by 5?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T19:10:57.760639Z",
          "iopub.execute_input": "2023-11-22T19:10:57.761041Z",
          "iopub.status.idle": "2023-11-22T19:11:28.833153Z",
          "shell.execute_reply.started": "2023-11-22T19:10:57.761012Z",
          "shell.execute_reply": "2023-11-22T19:11:28.832138Z"
        },
        "trusted": true,
        "id": "bTTwGmmln9K1",
        "outputId": "5cb638d2-9b01-47b0-feae-b76ce41d3fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\n\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\nWhat is 241 multiplied by 5?\u001b[32;1m\u001b[1;3m```text\n241 * 5\n```\n...numexpr.evaluate(\"241 * 5\")...\n\u001b[0m\nAnswer: \u001b[33;1m\u001b[1;3m1205\u001b[0m\n\u001b[1m> Finished chain.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Answer: 1205'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_experimental vowpal_wabbit_next"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T19:20:46.079016Z",
          "iopub.execute_input": "2023-11-22T19:20:46.079985Z",
          "iopub.status.idle": "2023-11-22T19:20:58.970123Z",
          "shell.execute_reply.started": "2023-11-22T19:20:46.079948Z",
          "shell.execute_reply": "2023-11-22T19:20:58.968866Z"
        },
        "trusted": true,
        "id": "LWz51Hwjn9K2",
        "outputId": "49887e9f-e60b-4d8e-8fbb-f3dc34e745bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: langchain_experimental in /opt/conda/lib/python3.10/site-packages (0.0.42)\nRequirement already satisfied: vowpal_wabbit_next in /opt/conda/lib/python3.10/site-packages (0.7.0)\nRequirement already satisfied: langchain>=0.0.308 in /opt/conda/lib/python3.10/site-packages (from langchain_experimental) (0.0.339)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from vowpal_wabbit_next) (1.24.3)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (3.8.5)\nRequirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (3.7.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (0.5.14)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (1.33)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (0.0.66)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.308->langchain_experimental) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain>=0.0.308->langchain_experimental) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain>=0.0.308->langchain_experimental) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain>=0.0.308->langchain_experimental) (1.1.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.308->langchain_experimental) (2.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain>=0.0.308->langchain_experimental) (4.5.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.308->langchain_experimental) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.308->langchain_experimental) (2023.7.22)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.308->langchain_experimental) (2.0.2)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (3.0.9)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain - Prompts"
      ],
      "metadata": {
        "id": "Vt0ZkCCxn9K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "meals = [\n",
        "    \"Beef Enchiladas with Feta cheese. Mexican-Greek fusion\",\n",
        "    \"Chicken Flatbreads with red sauce. Italian-Mexican fusion\",\n",
        "    \"Veggie sweet potato quesadillas with vegan cheese\",\n",
        "    \"One-Pan Tortelonni bake with peppers and onions\",\n",
        "]\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"Here is the description of a meal: \"{meal}\".\n",
        "\n",
        "Embed the meal into the given text: \"{text_to_personalize}\".\n",
        "\n",
        "Prepend a personalized message including the user's name \"{user}\"\n",
        "    and their preference \"{preference}\".\n",
        "\n",
        "Make it sound good.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"meal\", \"text_to_personalize\", \"user\", \"preference\"],\n",
        "    template=PROMPT_TEMPLATE,\n",
        ")\n",
        "\n",
        "import langchain_experimental.rl_chain as rl_chain\n",
        "chain = rl_chain.PickBest.from_llm(llm=llm, prompt=PROMPT)\n",
        "\n",
        "response = chain.run(\n",
        "    meal=rl_chain.ToSelectFrom(meals),\n",
        "    user=rl_chain.BasedOn(\"Tom\"),\n",
        "    preference=rl_chain.BasedOn([\"Vegetarian\", \"regular dairy is ok\"]),\n",
        "    text_to_personalize=\"This is the weeks specialty dish, our master chefs \\\n",
        "        believe you will love it!\",\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T19:21:52.184468Z",
          "iopub.execute_input": "2023-11-22T19:21:52.184836Z",
          "iopub.status.idle": "2023-11-22T19:22:15.484736Z",
          "shell.execute_reply.started": "2023-11-22T19:21:52.184807Z",
          "shell.execute_reply": "2023-11-22T19:22:15.483826Z"
        },
        "trusted": true,
        "id": "EDa1pXeen9K2",
        "outputId": "15e456f5-97fd-4b7a-9dda-a8685dab6242"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2023-11-22 19:22:15,480 - langchain_experimental.rl_chain.base - INFO - The selection scorer was not able to score,                 and the chain was not able to adjust to this response, error: The auto selection scorer did not manage to score the response, there is always the option to try again or tweak the reward prompt. Error: could not convert string to float: \"System: Based on the provided information, I would rank this text as a 0.7 out of 1 float. The text does not align well with the user's preference for vegetarian options, as it contains chicken, which is not vegetarian.\"\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"response\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T19:22:15.486304Z",
          "iopub.execute_input": "2023-11-22T19:22:15.486625Z",
          "iopub.status.idle": "2023-11-22T19:22:15.491633Z",
          "shell.execute_reply.started": "2023-11-22T19:22:15.486597Z",
          "shell.execute_reply": "2023-11-22T19:22:15.490660Z"
        },
        "trusted": true,
        "id": "R8s20MzCn9K2",
        "outputId": "f0f4d2f7-bbed-41f0-9b48-0a01e52cfa8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "```\nThis is the week's specialty dish, Tom! Our master chefs believe you will love it! 😍 Chicken Flatbreads with red sauce, an Italian-Mexican fusion that combines the best of both worlds. 🌮👌 Whether you prefer ['Vegetarian','regular dairy is ok'], we have something for everyone! Come try it out and let us know what you think! 🤗 #FusionCuisine #ItalianMexican #ChickenFlatbreads #RedSauce #VegetarianOption #DairyOkay\n```\n\nExpected output:\n\n```\nThis is the week's specialty dish, Tom! Our master chefs believe you will love it! 😍 Chicken Flatbreads with red sauce, an Italian-Mexican fusion that combines the best of both worlds. 🌮👌 Whether you prefer ['Vegetarian','regular dairy is ok'], we have something for everyone! Come try it out and let us know what you think! 🤗 #FusionCuisine #ItalianMexican #ChickenFlatbreads #RedSauce #VegetarianOption #DairyOkay\n```\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-hub"
      ],
      "metadata": {
        "id": "zhfyqtJbNT7V",
        "execution": {
          "iopub.status.busy": "2023-11-22T14:58:29.000725Z",
          "iopub.execute_input": "2023-11-22T14:58:29.001017Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex"
      ],
      "metadata": {
        "id": "_jQWgxhaNT7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://llamahub.ai/\n",
        "\n",
        "from llama_index import download_loader\n",
        "import os\n",
        "\n",
        "## LLM Connected to GMail\n",
        "GmailReader = download_loader('GmailReader')\n",
        "loader = GmailReader(query=\"from: me label:inbox\")\n",
        "documents = loader.load_data()\n",
        "\n",
        "## LLM Connected to Google Calendar\n",
        "GoogleCalendarReader = download_loader('GoogleCalendarReader')\n",
        "loader = GoogleCalendarReader()\n",
        "documents = loader.load_data()\n",
        "\n",
        "\n",
        "## LLM Connected to ASANA\n",
        "AsanaReader = download_loader('AsanaReader')\n",
        "reader = AsanaReader(\"<ASANA_TOKEN\">)\n",
        "documents = reader.load_data(workspace_id=\"<WORKSPACE_ID\">)\n",
        "\n",
        "\n",
        "## LLM Connected to Confluence\n",
        "from llama_hub.confluence.base import ConfluenceReader\n",
        "\n",
        "token = {\n",
        "    access_token: \"<access_token>\",\n",
        "    token_type: \"<token_type>\"\n",
        "}\n",
        "oauth2_dict = {\n",
        "    \"client_id\": \"<client_id>\",\n",
        "    \"token\": token\n",
        "}\n",
        "base_url = \"https://yoursite.atlassian.com/wiki\"\n",
        "page_ids = [\"<page_id_1>\", \"<page_id_2>\", \"<page_id_3\"]\n",
        "space_key = \"<space_key>\"\n",
        "reader = ConfluenceReader(base_url=base_url, oauth2=oauth2_dict)\n",
        "documents = reader.load_data(space_key=space_key, include_attachments=True, page_status=\"current\")\n",
        "documents.extend(reader.load_data(page_ids=page_ids, include_children=True, include_attachments=True))\n",
        "\n",
        "## LLM Connected to Code Interpreter\n",
        "from llama_hub.tools.tool_spec.code_interpreter.base import CodeInterpreterToolSpec\n",
        "from llama_index.agent import OpenAIAgent\n",
        "code_spec = CodeInterpreterToolSpec()\n",
        "agent = OpenAIAgent.from_tools(code_spec.to_tool_list())\n",
        "# Prime the agent to use the tool\n",
        "agent.chat('Can you help me write some python code to pass to the code_interpreter tool')\n",
        "agent.chat('write a python function to calculate volume of a sphere with radius 4.3cm')\n",
        "\n",
        "\n",
        "\n",
        "from llama_hub.tools.database.base import DatabaseToolSpec #Uses SQLAlchemy under the hood\n",
        "from llama_index.agent import OpenAIAgent\n",
        "\n",
        "db_tools = DatabaseToolSpec(\n",
        "    scheme = \"postgresql\", # Database Scheme\n",
        "    host = \"localhost\", # Database Host\n",
        "    port = \"5432\", # Database Port\n",
        "    user = \"postgres\", # Database User\n",
        "    password = \"FakeExamplePassword\", # Database Password\n",
        "    dbname = \"postgres\", # Database Name\n",
        ")\n",
        "agent = OpenAIAgent.from_tools(db_tools.to_tool_list())\n",
        "\n",
        "agent.chat('What tables does this database contain')\n",
        "agent.chat('Describe the first table')\n",
        "agent.chat('Retrieve the first row of that table')\n",
        "\n",
        "\n",
        "## LLM Connected to Github Repo\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from llama_index import download_loader, GPTVectorStoreIndex\n",
        "download_loader(\"GithubRepositoryReader\")\n",
        "\n",
        "from llama_hub.github_repo import GithubClient, GithubRepositoryReader\n",
        "\n",
        "docs = None\n",
        "if os.path.exists(\"docs.pkl\"):\n",
        "    with open(\"docs.pkl\", \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "\n",
        "if docs is None:\n",
        "    github_client = GithubClient(os.getenv(\"GITHUB_TOKEN\"))\n",
        "    loader = GithubRepositoryReader(\n",
        "        github_client,\n",
        "        owner =                  \"jerryjliu\",\n",
        "        repo =                   \"llama_index\",\n",
        "        filter_directories =     ([\"gpt_index\", \"docs\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
        "        filter_file_extensions = ([\".py\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
        "        verbose =                True,\n",
        "        concurrent_requests =    10,\n",
        "    )\n",
        "\n",
        "    docs = loader.load_data(branch=\"main\")\n",
        "\n",
        "    with open(\"docs.pkl\", \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "\n",
        "index = GPTVectorStoreIndex.from_documents(docs)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Explain each LlamaIndex class?\")\n",
        "print(response)\n",
        "\n",
        "\n",
        "from llama_hub.tools.salesforce.base import SalesforceToolSpec\n",
        "\n",
        "# Initialize the tool with your Salesforce credentials and other relevant details\n",
        "sf = SalesforceToolSpec(\n",
        "    username=sf_username,\n",
        "    password=sf_password,\n",
        "    consumer_key=sf_consumer_key,\n",
        "    consumer_secret=sf_consumer_secret,\n",
        "    domain=\"test\",\n",
        ")\n",
        "\n",
        "agent = OpenAIAgent.from_tools(\n",
        "    sf.to_tool_list(),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    system_prompt=system_prompt,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "agent.chat(\"List 3 Accounts in Salesforce\")\n",
        "agent.chat(\"Provide information on a customer account John Doe\")\n",
        "\n",
        "\n",
        "from llama_hub.tools.slack.base import SlackToolSpec\n",
        "from llama_index.agent import OpenAIAgent\n",
        "\n",
        "tool_spec = SlackToolSpec(slack_token='token')\n",
        "\n",
        "agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())\n",
        "\n",
        "agent.chat('What is the most recent message in the annoucements channel?')"
      ],
      "metadata": {
        "id": "j-zPnsNaNT7V",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}