{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 6603928,
          "sourceType": "datasetVersion",
          "datasetId": 3810072
        },
        {
          "sourceId": 6604037,
          "sourceType": "datasetVersion",
          "datasetId": 3810148
        }
      ],
      "dockerImageVersionId": 30497,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ff6c71bfb1646ad9ea16cbbb15b0cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e58fd536c5e4f39bc7bc6864d35e8cd",
              "IPY_MODEL_b958fb7493334369b054c07caf079c78",
              "IPY_MODEL_e2f3fa52a97c4d47b844d4beeb07f5d7"
            ],
            "layout": "IPY_MODEL_2e01b10346284803911b58a7c2843263"
          }
        },
        "0e58fd536c5e4f39bc7bc6864d35e8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_433b82e419bc4b36827aa332b7a617aa",
            "placeholder": "​",
            "style": "IPY_MODEL_e887d4b3d1d74494b69d7146dae23699",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b958fb7493334369b054c07caf079c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92abf8bac06a48c7a8223b0b649ede7e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13e01743be3141e0872659bf16b39e1f",
            "value": 2
          }
        },
        "e2f3fa52a97c4d47b844d4beeb07f5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fd6ee183e9e4d8995db48e2a1f0b947",
            "placeholder": "​",
            "style": "IPY_MODEL_dfb79aec839d45d8bdf817f78047d153",
            "value": " 2/2 [01:08&lt;00:00, 31.42s/it]"
          }
        },
        "2e01b10346284803911b58a7c2843263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "433b82e419bc4b36827aa332b7a617aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e887d4b3d1d74494b69d7146dae23699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92abf8bac06a48c7a8223b0b649ede7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13e01743be3141e0872659bf16b39e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fd6ee183e9e4d8995db48e2a1f0b947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfb79aec839d45d8bdf817f78047d153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.31.0 accelerate==0.21.0 einops==0.6.1 langchain==0.0.240 xformers==0.0.20 bitsandbytes==0.41.0 peft safetensors sentencepiece streamlit langchain sentence-transformers gradio pypdf chromadb==0.4.15 pypdfium2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T18:34:43.705068Z",
          "iopub.execute_input": "2023-11-21T18:34:43.705435Z",
          "iopub.status.idle": "2023-11-21T18:34:55.932895Z",
          "shell.execute_reply.started": "2023-11-21T18:34:43.705401Z",
          "shell.execute_reply": "2023-11-21T18:34:55.931707Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGD-5x9-NT7P",
        "outputId": "598647da-e595-4512-d513-02b7a48cdcac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: einops==0.6.1 in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: langchain==0.0.240 in /usr/local/lib/python3.10/dist-packages (0.0.240)\n",
            "Requirement already satisfied: xformers==0.0.20 in /usr/local/lib/python3.10/dist-packages (0.0.20)\n",
            "Requirement already satisfied: bitsandbytes==0.41.0 in /usr/local/lib/python3.10/dist-packages (0.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (0.0.66)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (2.8.7)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.240) (8.2.3)\n",
            "Requirement already satisfied: pyre-extensions==0.0.29 in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.20) (0.0.29)\n",
            "Collecting torch>=1.10.0 (from accelerate==0.21.0)\n",
            "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers==0.0.20) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers==0.0.20) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.7.91)\n",
            "Collecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.21.0)\n",
            "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.21.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.21.0) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (3.27.7)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (17.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.240) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.240) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.240) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.240) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.240) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.240) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.240) (3.20.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.240) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Installing collected packages: triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0\n",
            "    Uninstalling torch-2.1.0:\n",
            "      Successfully uninstalled torch-2.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained LLaMa2 Model"
      ],
      "metadata": {
        "id": "0gQQAxuWNT7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "hf_auth = '<hugging_face_access_token>'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T18:43:49.225602Z",
          "iopub.execute_input": "2023-11-21T18:43:49.226385Z",
          "iopub.status.idle": "2023-11-21T18:43:49.232437Z",
          "shell.execute_reply.started": "2023-11-21T18:43:49.226346Z",
          "shell.execute_reply": "2023-11-21T18:43:49.231376Z"
        },
        "trusted": true,
        "id": "C6NsBGmkNT7S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T18:43:50.078737Z",
          "iopub.execute_input": "2023-11-21T18:43:50.079553Z",
          "iopub.status.idle": "2023-11-21T18:43:50.085409Z",
          "shell.execute_reply.started": "2023-11-21T18:43:50.079508Z",
          "shell.execute_reply": "2023-11-21T18:43:50.084335Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dg2rCVfwNT7T",
        "outputId": "43e7301d-f70d-4d79-e486-352510589179"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T18:43:51.088929Z",
          "iopub.execute_input": "2023-11-21T18:43:51.089349Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "1ff6c71bfb1646ad9ea16cbbb15b0cec",
            "0e58fd536c5e4f39bc7bc6864d35e8cd",
            "b958fb7493334369b054c07caf079c78",
            "e2f3fa52a97c4d47b844d4beeb07f5d7",
            "2e01b10346284803911b58a7c2843263",
            "433b82e419bc4b36827aa332b7a617aa",
            "e887d4b3d1d74494b69d7146dae23699",
            "92abf8bac06a48c7a8223b0b649ede7e",
            "13e01743be3141e0872659bf16b39e1f",
            "6fd6ee183e9e4d8995db48e2a1f0b947",
            "dfb79aec839d45d8bdf817f78047d153"
          ]
        },
        "id": "jxVebPnxNT7T",
        "outputId": "e2ff5256-c238-4676-c80e-a255b44b0f1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ff6c71bfb1646ad9ea16cbbb15b0cec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA52r32QNT7T",
        "outputId": "b2702aa3-74dd-45dc-85a8-483dde32586d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_llama2 = transformers.pipeline(\n",
        "    model=model,# LLM model to be loaded\n",
        "    tokenizer=tokenizer, # A tokenizer receives a stream of characters, breaks it up into individual tokens (usually individual words)\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1)  # without this output begins repeating"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnqc-RQBNT7U",
        "outputId": "b82b93e2-1dbf-4bb6-d009-c4931cf72c39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.1.0+cu121)\n",
            "    Python  3.10.11 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain"
      ],
      "metadata": {
        "id": "gqU03jvjNT7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=ask_llama2)\n",
        "\n",
        "llm(prompt=\"What is Peak.ai?\").replace('\\n','')"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "MwtdkVNgNT7U",
        "outputId": "773194df-d062-4326-fa3b-04194e6ea642"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Peak.ai is an AI-powered platform that helps businesses automate their customer service operations by providing a chatbot that can answer customer queries and resolve issues in real-time. The platform uses natural language processing (NLP) and machine learning algorithms to understand customer requests and provide accurate responses.Peak.ai offers a range of features, including:1. Chatbot builder: Create custom chatbots using a drag-and-drop interface without any coding knowledge.2. Conversational AI: Use NLP and machine learning algorithms to understand customer requests and provide accurate responses.3. Integration with popular messaging platforms: Connect your chatbot to popular messaging platforms like Facebook Messenger, WhatsApp, and Slack.4. Advanced analytics: Track and analyze chatbot performance, user engagement, and sentiment analysis.5. Customizable workflows: Define custom workflows for your chatbot based on your business needs.6. Multi-language support: Offer support to customers in multiple languages using the same chatbot.7. Integration with third-party systems: Connect your chatbot to other systems such as CRM, ERP, or marketing automation tools.By leveraging Peak.ai's AI-powered platform, businesses can improve their customer service efficiency, reduce response times, and enhance customer satisfaction.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"Explain to me the difference between JAX and Numpy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "c411J1VHc3Kr",
        "outputId": "8f2041e9-f652-4778-9472-47533323a396"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Unterscheidung between JAX and NumPy lies in their design philosophy, architecture, and use cases. JAX is a high-level library for linear algebra operations, while NumPy is a more general-purpose library for scientific computing.\\n\\nJAX is designed to be fast and efficient for large-scale linear algebra operations, such as matrix multiplication, eigenvalue decomposition, and singular value decomposition. It achieves this through its use of specialized hardware acceleration, such as GPUs or TPUs, and its focus on parallelization and vectorization of linear algebra operations. JAX is particularly useful for tasks that require massive amounts of linear algebra computations, such as deep learning training or scientific simulations.\\n\\nOn the other hand, NumPy is a more general-purpose library for scientific computing, providing support for various data structures and mathematical operations beyond linear algebra. NumPy provides functions for array manipulation, numerical integration, optimization, signal processing, and more. While NumPy can also perform linear algebra operations, it does not have the same level of performance optimizations as JAX for large-scale linear algebra computations.\\n\\nIn summary, JAX is optimized for fast and efficient linear algebra computations, while NumPy provides a broader range of scientific computing capabilities. The choice between JAX and NumPy will depend on the specific needs of the project, including the scale and complexity of the linear algebra operations required.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "ukor4LY_zKU_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import chromadb\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFium2Loader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "GZh16yeCxvsx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "upload = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "mKWq-B_64f15",
        "outputId": "5cc8583f-2fdf-454e-881d-916fadd8f559"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-08c721ac-b64c-4780-9579-1d09341c2abe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-08c721ac-b64c-4780-9579-1d09341c2abe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving llama2_paper.pdf to llama2_paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load pdf files\n",
        "loader = PyPDFium2Loader(\"xgboost.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "QyPNSu078q9r"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the documents in small chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) #Chage the chunk_size and chunk_overlap as needed\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# specify embedding model (using huggingface sentence transformer)\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "#embed document chunks\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "# specify the retriever\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "yaCcGIJbxvpZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp-wKGVJ3lkn",
        "outputId": "61fc945b-1201-4818-82fe-b91f68d34ad4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain.vectorstores.chroma.Chroma at 0x7db1d6c46b00>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEDGmRm33nEb",
        "outputId": "753c0a8d-53bf-4138-f740-201a6d2d7994"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7db1d6c46b00>, search_type='similarity', search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conversation(query: str, chat_history: list) -> tuple:\n",
        "    try:\n",
        "\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history',\n",
        "            return_messages=False\n",
        "        )\n",
        "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            memory=memory,\n",
        "            get_chat_history=lambda h: h,\n",
        "        )\n",
        "\n",
        "        result = qa_chain({'question': query, 'chat_history': chat_history})\n",
        "        chat_history.append((query, result['answer']))\n",
        "        return '', chat_history\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history.append((query, e))\n",
        "        return '', chat_history"
      ],
      "metadata": {
        "id": "tQD8ddtyxvmA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"What is Weighted Quantile Sketch?\").replace('\\n','')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "jLiQZw-vBxHT",
        "outputId": "dc437f97-ed23-4b7c-a82c-12bd2c6e3738"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' nobody knows.Weighted quantile sketches are a generalization of the traditional quantile sketch, which is a data structure used to estimate statistical quantities such as percentiles or quantiles of a dataset. The traditional quantile sketch works by dividing the input data into fixed-size buckets based on their values, and then using these buckets to estimate the desired statistic. However, this approach can be computationally expensive when dealing with large datasets.A weighted quantile sketch addresses this issue by assigning different weights to each bucket based on their importance. This allows the algorithm to focus more resources on the buckets that contain the most informative data, while reducing the computational cost for less important buckets. As a result, weighted quantile sketches can provide accurate estimates of statistical quantities at a lower computational cost than traditional quantile sketches.The specific implementation details of a weighted quantile sketch will depend on the application and the type of data being analyzed. However, in general, a weighted quantile sketch consists of the following steps:1. Define the weights: The first step is to define the weights that will be assigned to each bucket. These weights can be based on various factors such as the importance of the data, the frequency of the data, or any other relevant characteristic.2. Divide the data: Next, divide the input data into fixed-size buckets based on their values. Each bucket will have a associated weight that reflects its importance.3. Estimate the statistic: For each bucket, compute an estimate of the desired statistical quantity (e.g., percentile, quantile) using the appropriate formula. The estimates from all buckets are then combined to obtain the final estimate of the statistical quantity.4. Refine the estimates: To improve the accuracy of the estimates, refine them using techniques such as bootstrapping or jackknife resampling.Some common applications of weighted quantile sketches include:* Estimating summary statistics of large datasets: Weighted quantile sketches can be used to efficiently estimate summary statistics such as mean, median, and standard deviation of a large dataset.* Computing percentiles and quantiles: Weighted quantile sketches can be used to quickly compute percentiles and quantiles of a dataset, which is useful in many applications such as quality control, anomaly detection, and risk assessment.* E'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "lBmvBIBLCQmp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('What is Weighted Quantile Sketch?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JD7bC3mCb-g",
        "outputId": "653c1f3e-3d13-4ccc-db0b-64c1e9d83c73"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is Weighted Quantile Sketch?',\n",
              " 'result': ' The weighted quantile sketch is a non-trivial weighted quantile summary structure that solves the problem of approximate quantile computation for weighted data. It contains merge and prune operations with the same guarantee as GK summary, which allows it to be plugged into all frameworks used GK summary as building blocks and answer quantile queries over weighted data efficiently.'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "    chatbot = gr.Chatbot(label='Chat with your data (Llama 7B )')\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    msg.submit(create_conversation, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "PYVEsocWxvir",
        "outputId": "c167c2a1-9dd7-4973-c467-87428896de23"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://20c2d192cc5a2674ea.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://20c2d192cc5a2674ea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://20c2d192cc5a2674ea.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-hub"
      ],
      "metadata": {
        "trusted": true,
        "id": "zhfyqtJbNT7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex"
      ],
      "metadata": {
        "id": "_jQWgxhaNT7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://llamahub.ai/\n",
        "\n",
        "from llama_index import download_loader\n",
        "import os\n",
        "\n",
        "## LLM Connected to GMail\n",
        "GmailReader = download_loader('GmailReader')\n",
        "loader = GmailReader(query=\"from: me label:inbox\")\n",
        "documents = loader.load_data()\n",
        "\n",
        "## LLM Connected to Google Calendar\n",
        "GoogleCalendarReader = download_loader('GoogleCalendarReader')\n",
        "loader = GoogleCalendarReader()\n",
        "documents = loader.load_data()\n",
        "\n",
        "\n",
        "## LLM Connected to ASANA\n",
        "AsanaReader = download_loader('AsanaReader')\n",
        "reader = AsanaReader(\"<ASANA_TOKEN\">)\n",
        "documents = reader.load_data(workspace_id=\"<WORKSPACE_ID\">)\n",
        "\n",
        "\n",
        "## LLM Connected to Confluence\n",
        "from llama_hub.confluence.base import ConfluenceReader\n",
        "\n",
        "token = {\n",
        "    access_token: \"<access_token>\",\n",
        "    token_type: \"<token_type>\"\n",
        "}\n",
        "oauth2_dict = {\n",
        "    \"client_id\": \"<client_id>\",\n",
        "    \"token\": token\n",
        "}\n",
        "base_url = \"https://yoursite.atlassian.com/wiki\"\n",
        "page_ids = [\"<page_id_1>\", \"<page_id_2>\", \"<page_id_3\"]\n",
        "space_key = \"<space_key>\"\n",
        "reader = ConfluenceReader(base_url=base_url, oauth2=oauth2_dict)\n",
        "documents = reader.load_data(space_key=space_key, include_attachments=True, page_status=\"current\")\n",
        "documents.extend(reader.load_data(page_ids=page_ids, include_children=True, include_attachments=True))\n",
        "\n",
        "## LLM Connected to Code Interpreter\n",
        "from llama_hub.tools.tool_spec.code_interpreter.base import CodeInterpreterToolSpec\n",
        "from llama_index.agent import OpenAIAgent\n",
        "code_spec = CodeInterpreterToolSpec()\n",
        "agent = OpenAIAgent.from_tools(code_spec.to_tool_list())\n",
        "# Prime the agent to use the tool\n",
        "agent.chat('Can you help me write some python code to pass to the code_interpreter tool')\n",
        "agent.chat('write a python function to calculate volume of a sphere with radius 4.3cm')\n",
        "\n",
        "\n",
        "\n",
        "from llama_hub.tools.database.base import DatabaseToolSpec #Uses SQLAlchemy under the hood\n",
        "from llama_index.agent import OpenAIAgent\n",
        "\n",
        "db_tools = DatabaseToolSpec(\n",
        "    scheme = \"postgresql\", # Database Scheme\n",
        "    host = \"localhost\", # Database Host\n",
        "    port = \"5432\", # Database Port\n",
        "    user = \"postgres\", # Database User\n",
        "    password = \"FakeExamplePassword\", # Database Password\n",
        "    dbname = \"postgres\", # Database Name\n",
        ")\n",
        "agent = OpenAIAgent.from_tools(db_tools.to_tool_list())\n",
        "\n",
        "agent.chat('What tables does this database contain')\n",
        "agent.chat('Describe the first table')\n",
        "agent.chat('Retrieve the first row of that table')\n",
        "\n",
        "\n",
        "## LLM Connected to Github Repo\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from llama_index import download_loader, GPTVectorStoreIndex\n",
        "download_loader(\"GithubRepositoryReader\")\n",
        "\n",
        "from llama_hub.github_repo import GithubClient, GithubRepositoryReader\n",
        "\n",
        "docs = None\n",
        "if os.path.exists(\"docs.pkl\"):\n",
        "    with open(\"docs.pkl\", \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "\n",
        "if docs is None:\n",
        "    github_client = GithubClient(os.getenv(\"GITHUB_TOKEN\"))\n",
        "    loader = GithubRepositoryReader(\n",
        "        github_client,\n",
        "        owner =                  \"jerryjliu\",\n",
        "        repo =                   \"llama_index\",\n",
        "        filter_directories =     ([\"gpt_index\", \"docs\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
        "        filter_file_extensions = ([\".py\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
        "        verbose =                True,\n",
        "        concurrent_requests =    10,\n",
        "    )\n",
        "\n",
        "    docs = loader.load_data(branch=\"main\")\n",
        "\n",
        "    with open(\"docs.pkl\", \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "\n",
        "index = GPTVectorStoreIndex.from_documents(docs)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Explain each LlamaIndex class?\")\n",
        "print(response)\n",
        "\n",
        "\n",
        "from llama_hub.tools.salesforce.base import SalesforceToolSpec\n",
        "\n",
        "# Initialize the tool with your Salesforce credentials and other relevant details\n",
        "sf = SalesforceToolSpec(\n",
        "    username=sf_username,\n",
        "    password=sf_password,\n",
        "    consumer_key=sf_consumer_key,\n",
        "    consumer_secret=sf_consumer_secret,\n",
        "    domain=\"test\",\n",
        ")\n",
        "\n",
        "agent = OpenAIAgent.from_tools(\n",
        "    sf.to_tool_list(),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    system_prompt=system_prompt,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "agent.chat(\"List 3 Accounts in Salesforce\")\n",
        "agent.chat(\"Provide information on a customer account John Doe\")\n",
        "\n",
        "\n",
        "from llama_hub.tools.slack.base import SlackToolSpec\n",
        "from llama_index.agent import OpenAIAgent\n",
        "\n",
        "tool_spec = SlackToolSpec(slack_token='token')\n",
        "\n",
        "agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())\n",
        "\n",
        "agent.chat('What is the most recent message in the annoucements channel?')"
      ],
      "metadata": {
        "id": "j-zPnsNaNT7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}